---
title: "Week3"
author: "Marcelo Szilagyi"
date: "11/16/2016"
output: html_document
---

```{r setup, include = FALSE, echo = FALSE, cache = TRUE, results='hide'}
knitr::opts_chunk$set(echo = TRUE, include=FALSE, cache = TRUE)
```

```{r load libraries, warning=FALSE, message=FALSE}
listpackages = c('data.table', 'tidytext', 'magrittr', 'dplyr', 'dtplyr', 'tibble', 'tm', 'stringr', 'ggplot2','scales','DT', 'tidyr', 'igraph','magrittr','gridExtra','readr')
loaded = suppressMessages(suppressWarnings(
  sapply(listpackages, function (x) library(x,character.only = T))
  ))

#devtools::install_github('hadley/ggplot2')
#devtools::install_github('thomasp85/ggforce')
#devtools::install_github('thomasp85/ggraph')
#devtools::install_github('slowkow/ggrepel')


rm(loaded)
```

```{r get_file}
# get the dataset from web and unzip it
#download.file('https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip',destfile = "temp")
#unzip('temp')
#file.remove('temp')
```

```{r join_all_to_a_corpo}
englishfiles = list.files(recursive = TRUE, pattern = glob2rx('*en_*.txt'))
filenames = str_extract(englishfiles,"(?<=.)[^.]+(?=.txt)")

# assign to create the 3 pieces of data
for (i in seq_along(englishfiles)) {
  assign(filenames[i], readLines(con = englishfiles[i],warn = FALSE)) }

# create tibbles
twitter = as_tibble(twitter);blogs = as_tibble(blogs);news = as_tibble(news)

# add row number and origin for future use
twitter = twitter %>% mutate(linenumber = row_number(), origin = "t")
blogs = blogs %>% mutate(linenumber = row_number(), origin = "b")
news = news %>% mutate(linenumber = row_number(), origin = "n")

# join all tibbles in one big tibble
corpo = bind_rows(twitter,news,blogs)


# create basic stats
names = c('blogs','news','twitter','corpo')
lines = c(nrow(blogs),nrow(news),nrow(twitter),nrow(corpo))
rm(twitter)
rm(blogs)
rm(news)

```

```{r count_words}

# Some words are more frequent than others - what are the distributions of word frequencies?
# from this 

# take the full corpo and count by origin 
wordbyword = corpo %>% unnest_tokens(word, value)
countwords = wordbyword %>% count(origin, word, sort = TRUE) %>% ungroup()
totalwords = countwords %>% group_by(origin) %>% summarize(total = sum(n))
countwords = left_join(countwords,totalwords, by = 'origin')
countwords = countwords %>% group_by(origin) %>% mutate(cumulative = cumsum(n/total), count = dense_rank(desc(n))) %>% ungroup()
words = rbind(totalwords[2],sum(totalwords[2])); colnames(words) = "words"

# this is temp database only to show 
temp = countwords %>% filter(cumulative<0.95) %>% mutate(origin = ifelse(origin =='t',"Twitter", ifelse(origin =='b',"Blog","News")))

midinfo = temp %>% filter(round(cumulative,2)==(0.50)) %>% select(c(origin,cumulative,count)) %>% group_by(origin)%>% ungroup() %>% filter(count==max(count)|count==min(count)) %>% group_by(origin) %>% filter(cumulative==max(cumulative))

parinfo = temp %>% filter(round(cumulative,2)==(0.80)) %>% select(c(origin,cumulative,count)) %>% group_by(origin)%>% ungroup() %>% filter(count==max(count)|count==min(count)) %>% group_by(origin) %>% filter(cumulative==max(cumulative))

almostallinfo = temp %>% filter(round(cumulative,2)==(0.950)) %>% select(c(origin,cumulative,count)) %>% group_by(origin)%>% ungroup() %>% filter(count==max(count)|count==min(count)) %>% group_by(origin) %>% filter(cumulative==max(cumulative))

# 100 top words 
bestwords = temp %>% group_by(word) %>% summarize(n2 = sum(n), total2 = sum(total), per2 = cumsum(n2/total2)) %>% ungroup() %>% arrange(desc(per2)) %>% filter(row_number()<101) %>% select(word) %>% mutate(rank = cut(row_number(),breaks = 10,labels = FALSE))

first20 = bestwords[1:20,1]; colnames(first20) <- "1st to 20th"
`21_to_40` = bestwords[21:40,1]; colnames(`21_to_40`) <-  "21st to 40th"
`41_to_60` = bestwords[41:60,1]; colnames(`41_to_60`) <-  "41st to 60th"
`61_to_80` = bestwords[61:80,1]; colnames(`61_to_80`) <-  "61st to 80th"
`81_to_100` = bestwords[81:100,1]; colnames(`81_to_100`) <-  "80th to 100th"

printbestwords = datatable(cbind(first20,`21_to_40`,`41_to_60`,`61_to_80`,`81_to_100` ),autoHideNavigation = TRUE,rownames = FALSE)
uniquewordscount = countwords %>% group_by(origin) %>% summarize(n_distinct(word))  
```

```{r create_plots_for_count_words }
# prep for the graph of density- (ideas from <http://tidytextmining.com/tfidf.html>)

originnames = c('t'="Twitter",'b'="Blogs",'n'='News')

plotfrequency = ggplot(countwords, aes(n/total, fill = origin)) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  xlim(NA,0.0000035) +
  labs(title = "Term Frequency Distribution") +
  facet_wrap(~origin, scales = "free_y",labeller = as_labeller(originnames),nrow = 3)+
  scale_y_continuous(labels = comma) +
  expand_limits(x = 0, y = 0)

a = ggplot(data= temp ,aes(x=count, y=cumulative, color=origin)) + geom_point(shape=1) + 
  scale_x_continuous(trans = 'log10', breaks =c( 10,100,300,500,1000,3000,10000))  + 
  scale_y_continuous(breaks = seq(0.05,0.95, by= 0.1), labels = percent) + 
  labs (title  = "Words neeeded to all word instances") + 
  ylab ("Word instance %") + 
  xlab("Number of words") +
  theme(legend.position="bottom")

# annotation to display the number of words
a = a +  annotate("rect", xmin = midinfo$count[1], xmax = midinfo$count[2], ymin = 0.475, ymax = 0.525, alpha = .2)  
a = a +  annotate("text", x= midinfo$count[1], y=midinfo$cumulative[1], label=paste0(as.character(midinfo[1,1])," ",as.character(midinfo[1,3])," words \n",as.character(midinfo[2,1])," ",as.character(midinfo[2,3])," words "),size =3)

a = a +  annotate("rect", xmin = parinfo$count[1], xmax = parinfo$count[2], ymin = 0.775, ymax = 0.825, alpha = .2)  
a = a +  annotate("text", x= parinfo$count[1], y=parinfo$cumulative[1], label=paste0(as.character(parinfo[1,1])," ",as.character(parinfo[1,3])," words \n",as.character(parinfo[2,1])," ",as.character(parinfo[2,3])," words "), size =3)

a = a +  annotate("rect", xmin = almostallinfo$count[1], xmax = almostallinfo$count[2], ymin = 0.925, ymax = 0.975, alpha = .2)  
a = a +  annotate("text", x= almostallinfo$count[1]-1000, y=almostallinfo$cumulative[1], label=paste0(as.character(almostallinfo[1,1]),"  ",as.character(almostallinfo[1,3])," words \n ",as.character(almostallinfo[2,1])," ",as.character(almostallinfo[2,3])," words "),size =3)

```

```{r 2ngrams}

# need to reduce the size of the database - I reduced to 10% of the size
ifelse(nrow(corpo)>1e+6, 
       corpo1 = corpo %>% group_by(origin) %>% sample_frac(0.1) %>% ungroup(),
       corpo1 = corpo)

bigram = corpo1 %>% unnest_tokens(bigram,value,token = "ngrams", n=2)
splitwords = as_tibble(str_split(bigram$bigram," ",simplify = TRUE))
bigram  = bind_cols(bigram, splitwords) %>% rename(word1 = V1, word2 = V2)
rm(splitwords)

# basic counting
countingbigrams = count(bigram,bigram,word1,word2,sort=TRUE)

# some easy graphic from <http://tidytextmining.com/ngrams.html>
bigram_graph <- countingbigrams  %>% ungroup() %>% select(word1,word2,n) %>%
  top_n (n= 100, wt = n) %>%
  graph_from_data_frame()

maintwo = "Bigrams"
subtwo = paste("relationship of the top","bigrams" )

# frequency
total = sum(countingbigrams$n)
countingbigrams = countingbigrams %>% ungroup() %>% mutate(cumulative = cumsum(n/total))

#pareto graph
temp <- countingbigrams %>% mutate(number_bigrams  = row_number(cumulative)) %>% select(number_bigrams,cumulative) %>% filter(number_bigrams %in% c(2^(0:20)))

modelbi <- loess(cumulative ~ number_bigrams, data = temp)

bigraph  = ggplot(data=temp ,aes(x=number_bigrams, y=cumulative)) + geom_point(shape=1)
bigraph  = bigraph  +  scale_y_continuous(breaks = seq(0.0,1, by= 0.1), labels = percent) + 
  labs (title  = "Bigrams cumulative frequency") + 
  ylab ("Cumulative instance %") + 
  xlab("Number of Bigrams") +
  theme(legend.position="bottom")+
  scale_x_continuous(trans = 'log2',labels = comma, breaks = 4^(0:10) )+
  stat_smooth(method = "loess")


```

```{r 3ngrams}

# 3 grams
trigram = corpo1 %>% unnest_tokens(trigram ,value,token = "ngrams", n=3)
trisplitwords = as_tibble(str_split(trigram$trigram," ",simplify = TRUE))
trigram  = bind_cols(trigram, trisplitwords) %>% rename(word1 = V1, word2 = V2, word3 = V3)
rm(trisplitwords)

# basic counting
countingtrigrams = count(trigram, trigram, word1, word2, word3, sort=TRUE)

# graph
trigram_graph <- countingtrigrams %>% ungroup() %>% select(word1,word2, word3, n) %>% 
  top_n (n= 60, wt = n) %>%
  graph_from_data_frame()
maintree = "Trigrams"
subtree = paste("relationship of the top", "trigrams" )

# frequency
total = sum(countingtrigrams$n)
countingtrigrams = countingtrigrams %>% ungroup() %>% mutate(cumulative = cumsum(n/total))

#pareto graph
temp2 <- countingtrigrams %>% mutate(number_trigrams  = row_number(cumulative)) %>% select(number_trigrams,cumulative) %>% filter(number_trigrams %in% c(2^(0:20)))

trigraph  = ggplot(data=temp2 ,aes(x=number_trigrams, y=cumulative)) + geom_point(shape=1)
trigraph  = trigraph  +  scale_y_continuous(breaks = seq(0.0,1, by= 0.1), labels = percent) + 
  labs (title  = "Trigrams cumulative frequency") + 
  ylab ("Cumulative instance %") + 
  xlab("Number of Trigrams") +
  theme(legend.position="bottom")+
  scale_x_continuous(trans = 'log2',labels = comma, breaks = 4^(0:10) )+
  stat_smooth(method = "loess")

```

```{r countingnonenglish}

#get the english dictionary
download.file('https://raw.githubusercontent.com/dwyl/english-words/master/words3.txt',destfile = "dictionary.txt")
dictionary = read_csv("dictionary.txt",col_names = "word")
dictionary  = dictionary %>% mutate(dictionary="dictionary")
problemsummary = data.frame()


wordcountcorpo1 = corpo1 %>% unnest_tokens(word ,value) %>% count(word, sort = TRUE) %>% ungroup()
p0n=nrow(wordcountcorpo1)
ref = sum(wordcountcorpo1$n)
   p0e = wordcountcorpo1 %>% select(word) %>% sample_n(20)
   p0c = 100

   # problem number 1 - the 's' problem
   p1n = sum(str_count(pattern = "[`’‘]",string = wordcountcorpo1$word))
   p1e = wordcountcorpo1 %>% filter(grepl("[`’‘]", word)) %>% sample_n(20)
   p1c = wordcountcorpo1 %>% filter(grepl("[`’‘]", word)) %>% summarize(nn= 100*sum(n)/ref)
   wordcountcorpo1$word = gsub(pattern = "[`’‘]",replacement = "'", x = wordcountcorpo1$word)
   
   # problem number 2 - only numbers
   numbers = "^-?\\d+(,\\d+)*(\\.\\d+(e\\d+)?)?$"
   p2n = sum(str_count(pattern = numbers ,string = wordcountcorpo1$word))
   p2e = wordcountcorpo1 %>% filter(grepl(numbers, word)) %>% sample_n(20,replace = FALSE)
   p2c = wordcountcorpo1 %>% filter(grepl(numbers, word)) %>% summarize(nn= 100*sum(n)/ref)
   wordcountcorpo1 <- wordcountcorpo1 %>% filter(!grepl(pattern = numbers, x = wordcountcorpo1$word))

   # problem number 3 - web references
   web = "\\w\\.{1}\\D{2,3}$"
   p3n = sum(str_count(pattern = web ,string = wordcountcorpo1$word))
   p3e = wordcountcorpo1 %>% filter(grepl(web, word)) %>% sample_n(20)
   p3c = wordcountcorpo1 %>% filter(grepl(web, word)) %>% summarize(nn= 100*sum(n)/ref)
   wordcountcorpo1 <- wordcountcorpo1 %>% filter(!grepl(pattern = web, x = wordcountcorpo1$word))
   
   #problem number 4 - numbers in the middle of words
   mid = "\\d"
   p4n = sum(str_count(pattern = mid ,string = wordcountcorpo1$word))
   p4e = wordcountcorpo1 %>% filter(grepl(mid, word)) %>% sample_n(20)
   p4c = wordcountcorpo1 %>% filter(grepl(mid, word)) %>% summarize(nn= 100*sum(n)/ref)  
   wordcountcorpo1 <- wordcountcorpo1 %>% filter(!grepl(pattern = mid, x = wordcountcorpo1$word))
   
   #problem number 5 - words with foreign character()
   
   foreign = "[^0-9A-Za-z\\.']"
   p5n = sum(str_count(pattern = foreign ,string = wordcountcorpo1$word))
   p5e = wordcountcorpo1 %>% filter(grepl(foreign, word)) %>% sample_n(20)
   p5c = wordcountcorpo1 %>% filter(grepl(foreign, word)) %>% summarize(nn= 100*sum(n)/ref)     
   wordcountcorpo1 <- wordcountcorpo1 %>% filter(!grepl(pattern = foreign, x = wordcountcorpo1$word))
   
  # dictionary removal
   
  nonenglishwordcountcorpo1 <- wordcountcorpo1 %>% anti_join(dictionary,by = "word") %>% ungroup() %>% arrange(desc(n))
  p6n = nrow(nonenglishwordcountcorpo1)
  p6e = sample_n(nonenglishwordcountcorpo1,20)
  p6c = sum(nonenglishwordcountcorpo1$n)/ref*100
  
  englishwordcountcorpo1 <- wordcountcorpo1 %>% inner_join(dictionary,by = "word") %>% ungroup() %>% arrange(desc(n))
  p7n = nrow(englishwordcountcorpo1)
  p7e = sample_n(englishwordcountcorpo1,20)
  p7c = sum(englishwordcountcorpo1$n)/ref*100
  

Actions = c("Original Base","Wrong format","Only numbers","Web References","Word Number mix","Foreign chars","Not on dictionary","Dictionary words")
Cases = c(p0n,p1n,p2n,p3n,p4n,p5n,p6n,p7n)

Examples = c(paste(p0e$word,collapse=" ,"),paste(p1e$word,collapse=" ,"),paste(p2e$word,collapse=" ,"),paste(p3e$word,collapse=" ,"),paste(p4e$word,collapse=" ,"),paste(p5e$word,collapse=" ,"),paste(p6e$word,collapse=" ,"),paste(p7e$word,collapse=" ,"))

Instance = unlist(c(p0c,p1c,p2c,p3c,p4c,p5c,p6c,p7c))

SummaryCleaning = cbind.data.frame(Actions,Cases,Instance,Examples)
SummaryCleaning = SummaryCleaning %>% mutate(Word_Per = Cases/max(Cases)*100)

```

```{r loss of bigrams}
nrow(bigram)



```



  
####################################################################

# Exploratory Data Analysis for the Data Science Capstone Project

The exploratory data analysis for the english related data provided [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) starts with preprocessing of the data: the data is uncompressed (unziped), and one large vector is created for each database avaialable - Blogs, Newspapers and Twitter messages. The files are read and converted to a `tibble` - one tipe of `data.frame`. All databases are then joined using a package called `tidytext`. To reduce processing of this document, a 'tidy' corpora (one word per line) was created and is avaiable at [TODO].

Here are some of the basic information about the databases. In general, the consolidated database has more than the estimated [500,000 words in English] (http://www.slate.com/articles/life/the_good_word/2006/04/word_count.html)

```{r, echo = FALSE, results='markup'}
datatable(cbind.data.frame(names,lines,words,unique_words = uniquewordscount[,2]),colnames = c('Database','Lines','Words','Unique Words')) %>% formatCurrency(c('lines','words','n_distinct(word)'),currency = "",digits = 0)
```

The plots for all the databases show a similar result - most of the words are lightly used, while some words are recurrently used.  
```{r, echo = FALSE, cache = TRUE, results= 'markup', warning= FALSE, message=FALSE}
plotfrequency
```

As it is possible to see on the graph below, the 3 databases have the same trend in terms of coverage - a small number of words are responsible for a large covergate of instances. 
For example, 50% of instances are covered by 110 words in the blog, while 228 words in the news database. Only 1417 words cover 80% of all instances in Twitter, while 2357 words are required for news. 
This demonstrates that though the trend is the same - a small number of words cover a large part of instances - the 'news' database requires more words to get to the same coverage. One reason could be type of content  - news are potentially more complex than twitter messages.

```{r graph all word instances, results='markup'}
a
```

The top 100 words for all consolidated databases are displayed below - masculine and feminine pronoums have very distinct frequencies: 

```{r, results='markup'}
printbestwords
```


```

2) What are the frequencies of 2-grams and 3-grams in the dataset?

Using the package 'tidytext', 2 and 3 grams were created, using a sample of 10% of all the databases. This sample shows the that 64 bigrams account for aprox. 5% of all bigrams instances. As words are combined, the factorial growth of potential can be noted - 256 bigrams are required to reach 10% of the coverage (4 times more word only doubling the coverage). More than 1024 bigrams are required to cover 20%, while to reach 40%, more than 16 thousand bigrams are required (16 times the number of bigrams to double the coverage from 20% to 40%). 
Trigrams, on their turn, show a similar trend, but with a smaller coverage. 256 trigrams do not reach 2.5% of instances, and 16 thousand combinations reach only 11% of the coverage (compared to 40% of bigrams)

The graph below displays the cumulative frequency of bigrams and trigrams: 

```{r, results='asis', echo=FALSE}
 library(gridExtra)
 grid.arrange(bigraph, trigraph, ncol=2)
```

It is possible to arrange bigrams and trigrams in network of words, as shown below. By looking the Bigram and Trigram network of more frequent words and the arrows, it is possible to see the usual relationship between words and expansions to n-grams of 4 and 5 workds, for example:
 - 'I'm going to be able'
 - 'this is going to be'
 - 'thanks for the best'
 - 'I will be a great'

A future model can explore those relantionships and organize words by their proximity (clusters)

```{r bigramplot}
plot(bigram_graph , vertex.size=13, edge.arrow.size=0.2, main=maintwo, sub = subtwo)
```

```{r trigramplot}
plot(trigram_graph, vertex.size=13, edge.arrow.size=0.2, main=maintree, sub = subtree)
```


4) How do you evaluate how many of the words come from foreign languages?

In order to clean the database (from foreign languagues and other misplaced character), a 5 step process was applied. 
 - 1. Replace symbols that look like the apostrophe - like the ``’‘`
 - 2. Remove words that in fact are just numbers
 - 3. Remove words that are references to webpages
 - 4. Remove words that have digits (e.g.: 2do, 2night)
 - 5. Remove words that appear to have characthers that would represent accents in other languagues than English
 - 6. Remove words that do not belong to a dictionary

A summary of the results of each operation is displayed on the table below. Note that words that were not part of the dictionary represent 55% of the base, but account only for aprox. 4% of the coverage. In constrast, dictionary based words represent 32% of the words, and 93% of all coverage. 
   

```{r}
datatable(SummaryCleaning[c(1,2,5,3,4)],colnames = c('Step/Issue','Number of Words (Cases)','Word (%)','Coverage (%)','Example')) %>% formatCurrency('Cases',currency = "",digits = 0) %>% formatCurrency(c('Instance','Word_Per'), currency = "", digits = 2)
```

5) Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words
in the dictionary to cover the same number of phrases?

One way to think about the coverage is to understand the loss of bigrams by excluding non dictionary words.

```{r}

```

